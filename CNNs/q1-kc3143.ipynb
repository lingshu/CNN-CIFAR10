{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "#             print (img_arr.shape)\n",
    "        img_arr = get_img_array(f)\n",
    "#         img_arr = get_img_array(f).reshape([-1, 32, 32, 3])\n",
    "        img_arr = np.float32(img_arr / 255.0)\n",
    "        images.append(img_arr)\n",
    "    images = np.stack(images)\n",
    "    print (images.shape)\n",
    "    return images\n",
    "\n",
    "def get_all_paths(folder):\n",
    "    \"\"\"\n",
    "    return a list of directory of all imgs in folder\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    paths = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        paths.append(folder + f)\n",
    "    \n",
    "    return paths\n",
    "    \n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "#     paths = get_all_paths(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)\n",
    "    \n",
    "def get_batch(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Return minibatch of samples and labels\n",
    "        \n",
    "    :param X, y: samples and corresponding labels\n",
    "    :parma batch_size: minibatch size\n",
    "    :returns: (tuple) X_batch, y_batch\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    index = np.random.randint(0, m - batch_size)\n",
    "    X_batch = X[index:(index + batch_size)]\n",
    "    y_batch = y[index:(index + batch_size)]\n",
    "\n",
    "    return X_batch, y_batch\n",
    "\n",
    "def Z_ScoreNorm(x):\n",
    "    mu = np.average(x)\n",
    "    std = np.std(x)\n",
    "    x=(x-mu)/std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Right now we have the following strcuture: Conv1 -> Norm1 -> Pool1 -> Conv2 -> Norm2 -> Pool2 -> Dense \n",
    "# -> Dense -> Logits\n",
    "def cnn_model_fn(features):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer; has size batchsize*32*32*3\n",
    "    with tf.variable_scope(\"data\"):\n",
    "        input_layer = tf.reshape(features, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Initialize a regularizer, using L2 regularization\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "    # Convolutional Layer #1\n",
    "    # output size batchsize*32*32*32\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=input_layer,\n",
    "            filters = 32,\n",
    "            kernel_size = [5, 5],\n",
    "            padding='same',\n",
    "            activation = tf.nn.relu,\n",
    "            kernel_regularizer=regularizer)\n",
    "    # Normalization layer #1:  output size batchsize*32*32*32\n",
    "    with tf.variable_scope(\"norm1\"):\n",
    "#         norm1 = tf.layers.batch_normalization(inputs=conv1)\n",
    "        norm1 = tf.nn.lrn(conv1)\n",
    "    \n",
    "    # Pooling Layer #1; output size batchsize*16*16*32\n",
    "    with tf.variable_scope(\"pool1\"):\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=norm1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    # conv2 output size: batchsize*16*16*64\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=64,\n",
    "            kernel_size=[5,5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_regularizer=regularizer)\n",
    "    \n",
    "     # norm2 output size: batchsize*16*16*64\n",
    "    with tf.variable_scope(\"norm2\"):\n",
    "#         norm2 = tf.layers.batch_normalization(inputs=conv2)\n",
    "        norm2 = tf.nn.lrn(conv2)\n",
    "    \n",
    "    # pool2 output size: batchsize*8*8*64\n",
    "    with tf.variable_scope(\"pool2\"):\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=norm2, pool_size=[2,2], strides = 2)\n",
    "    \n",
    "    # Dense Layer\n",
    "    # pool2_flat size: batchsize*1024 \n",
    "    with tf.variable_scope(\"fully_connect1\"):\n",
    "        pool2_flat = tf.reshape(pool2, [-1, 8*8*64])\n",
    "    # dense1 size: batchsize*512\n",
    "    with tf.variable_scope(\"fully_connect2\"):\n",
    "        dense1 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu, kernel_regularizer=regularizer)\n",
    "    \n",
    "#     # dense2 size: batchsize*128\n",
    "#     with tf.variable_scope(\"fully_connect3\"):\n",
    "#         dense2 = tf.layers.dense(inputs=dense1, units=128, activation=tf.nn.relu)\n",
    "    # dropoutï¼Œ we will not use dropout here\n",
    "    \n",
    "    # Logits Layer\n",
    "    with tf.variable_scope(\"logits\"):\n",
    "        y_conv = tf.layers.dense(inputs=dense1, units=10)\n",
    "    \n",
    "    global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "    \n",
    "    return y_conv, global_step\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'horse': 7, 'automobile': 1, 'deer': 4, 'dog': 5, 'frog': 6, 'cat': 3, 'truck': 9, 'ship': 8, 'airplane': 0, 'bird': 2}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "(50000, 32, 32, 3)\n",
      "Loaded 10000/10000\n",
      "(10000, 32, 32, 3)\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = '/Users/mrdoggie/Desktop/Columbia/deepLearning/HW1/cifar10-hw1/'\n",
    "train_data, train_labels = get_train_data(data_root_path) # this may take a few minutes\n",
    "# test_data_paths = get_images(data_root_path + 'test')\n",
    "\n",
    "# normalize the data before training.\n",
    "train_data = Z_ScoreNorm(train_data)\n",
    "# arrange 10% of the training data to validation set\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "indexs = np.random.choice(50000, 5000, replace=False)\n",
    "valid_data = train_data[indexs]\n",
    "valid_labels = train_labels[indexs]\n",
    "train_data = np.delete(train_data,indexs, axis = 0)\n",
    "train_labels = np.delete(train_labels,indexs)\n",
    "\n",
    "print('Data loading done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graph to: /Users/mrdoggie/Desktop/Columbia/deepLearning/HW2/part1-final-version/\n",
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from /Users/mrdoggie/Desktop/Columbia/deepLearning/HW2/part1-final-version/-5000\n",
      "Restored checkpoint from: /Users/mrdoggie/Desktop/Columbia/deepLearning/HW2/part1-final-version/-5000\n",
      "step: 5000, training accuracy: 0.875, loss: 0.321993, validation accuracy: 0.723400\n",
      "Saved checkpoint.\n",
      "step: 5500, training accuracy: 0.90625, loss: 0.313460, validation accuracy: 0.747000\n",
      "Saved checkpoint.\n",
      "step: 6000, training accuracy: 0.921875, loss: 0.288588, validation accuracy: 0.751600\n",
      "Saved checkpoint.\n",
      "step: 6500, training accuracy: 0.9375, loss: 0.233372, validation accuracy: 0.754400\n",
      "Saved checkpoint.\n",
      "step: 7000, training accuracy: 0.898438, loss: 0.332599, validation accuracy: 0.753200\n",
      "Saved checkpoint.\n",
      "step: 7500, training accuracy: 0.96875, loss: 0.182665, validation accuracy: 0.755200\n",
      "Saved checkpoint.\n",
      "step: 8000, training accuracy: 0.921875, loss: 0.232337, validation accuracy: 0.757400\n",
      "Saved checkpoint.\n",
      "Final Accuracy on Validation Set: 0.7558\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, 32,32,3])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "y_conv, global_step = cnn_model_fn(x)\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=y_conv)\n",
    "tf.summary.scalar(\"Total_loss\", loss)\n",
    "\n",
    "with tf.variable_scope('GD_optimizer'):\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=0.02)\n",
    "    gradients = opt.compute_gradients(loss)\n",
    "    train_step = opt.apply_gradients(gradients, global_step=global_step)\n",
    "for gv in gradients:\n",
    "    namespace = str(gv[1])\n",
    "    if \"conv1/conv2d/kernel:0\" in namespace:\n",
    "        tf.summary.histogram(\"Gradients/conv1\", gv[0])\n",
    "    if \"conv2/conv2d/kernel:0\" in namespace:\n",
    "        tf.summary.histogram(\"Gradients/conv2\", gv[0])\n",
    "    if \"fully_connect2/dense/kernel:0\" in namespace:\n",
    "        tf.summary.histogram(\"Gradients/last_fully_connected\", gv[0])\n",
    "        \n",
    "#     train_step = opt.minimize(loss, global_step=global_step)\n",
    "# print (tf.trainable_variables())\n",
    "# tf.summary.histogram(\"Gradients\", tf.gradients(loss, 'conv1/conv2d/kernel:0'))\n",
    "\n",
    "with tf.variable_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)\n",
    "tf.summary.scalar(\"Accuracy/train\", accuracy)\n",
    "\n",
    "merged=tf.summary.merge_all()\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    SAVE_PATH = \"/Users/mrdoggie/Desktop/Columbia/deepLearning/HW2/part1-final-version/\"\n",
    "    print('Saving graph to: %s' % SAVE_PATH)\n",
    "    train_writer = tf.summary.FileWriter(SAVE_PATH)\n",
    "    train_writer.add_graph(tf.get_default_graph())\n",
    "    \n",
    "    try:\n",
    "        print(\"Trying to restore last checkpoint ...\")\n",
    "        last_chk_path=tf.train.latest_checkpoint(checkpoint_dir=SAVE_PATH)\n",
    "        saver.restore(sess, save_path=last_chk_path)\n",
    "        print(\"Restored checkpoint from:\", last_chk_path)\n",
    "    except:\n",
    "        print(\"Failed to restore checkpoint. Restarting...\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    one_hot_valid_labels = tf.one_hot(indices=tf.cast(valid_labels, tf.int32), depth=10)\n",
    "    one_hot_valid_labels = sess.run(one_hot_valid_labels)\n",
    "    for i in range(3001):\n",
    "        x_batch, y_batch = get_batch(train_data, train_labels, 128)\n",
    "        y_batch = tf.one_hot(indices=tf.cast(y_batch, tf.int32), depth=10)\n",
    "        y_batch = sess.run(y_batch)\n",
    "\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            data_merged, num_step, train_accuracy, train_loss = sess.run([merged, global_step, accuracy, loss], feed_dict={\n",
    "                x: x_batch,\n",
    "                y: y_batch\n",
    "            })\n",
    "            \n",
    "            valid_accuracy = accuracy.eval(feed_dict={\n",
    "                x: valid_data, y: one_hot_valid_labels})\n",
    "#             train_accuracy, train_loss = sess.run([accuracy, loss], feed_dict={\n",
    "#                 x: x_batch,\n",
    "#                 y: y_batch\n",
    "#             })\n",
    "#             train_accuracy = accuracy.eval(feed_dict={\n",
    "#                 x: x_batch,\n",
    "#                 y: y_batch})\n",
    "#             train_loss = loss.eval(feed_dict={\n",
    "#                 x: x_batch,\n",
    "#                 y: y_batch})\n",
    "            print('step: %d, training accuracy: %g, loss: %f, validation accuracy: %f' % (num_step, train_accuracy, train_loss, valid_accuracy))\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"Accuracy/validation\", simple_value=valid_accuracy),\n",
    "            ])\n",
    "            train_writer.add_summary(data_merged, num_step)\n",
    "            train_writer.add_summary(summary, num_step)\n",
    "            # save the process\n",
    "            saver.save(sess, save_path=SAVE_PATH, global_step=global_step)\n",
    "            print(\"Saved checkpoint.\")\n",
    "        train_step.run(feed_dict={x: x_batch, y: y_batch})\n",
    "        \n",
    "\n",
    "    print('Final Accuracy on Validation Set: %g' % accuracy.eval(feed_dict={\n",
    "        x: valid_data, y: one_hot_valid_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5000 steps with step size = 0.05\n",
    "# followed with 3000 step size = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/10000\n",
      "(10000, 32, 32, 3)\n",
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from /Users/mrdoggie/Desktop/Columbia/deepLearning/HW2/part1-final-version/-8000\n",
      "Restored checkpoint from: /Users/mrdoggie/Desktop/Columbia/deepLearning/HW2/part1-final-version/-8000\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "x_test = get_images(data_root_path + 'test')\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        print(\"Trying to restore last checkpoint ...\")\n",
    "        last_chk_path=tf.train.latest_checkpoint(checkpoint_dir=SAVE_PATH)\n",
    "        saver.restore(sess, save_path=last_chk_path)\n",
    "        print(\"Restored checkpoint from:\", last_chk_path)\n",
    "    except:\n",
    "        print(\"Failed to restore checkpoint. Restarting...\")\n",
    "        \n",
    "    predict_raw = y_conv.eval(feed_dict={x: x_test})\n",
    "    predict_raw = predict_raw.T\n",
    "    print (predict_raw.shape)\n",
    "    save_predictions('ans1-jw3564.npy', predict_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-jw3564.npy')\n",
    "print(loaded_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
